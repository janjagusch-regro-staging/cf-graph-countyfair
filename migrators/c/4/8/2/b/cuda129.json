{
  "__migrator__": true,
  "args": [
    "migrator_ts: 1738229377\n__migrator:\n  kind:\n    version\n  migration_number:\n    1\n  build_number:\n    1\n  paused: false\n  override_cbc_keys:\n    - cuda_compiler_stub\n  check_solvable: false\n  primary_key: cuda_compiler_version\n  ordering:\n    cuda_compiler_version:\n      - 12.4\n      - 12.6\n      - 12.8\n      - None\n      - 12.9\n      # to allow manual opt-in for CUDA 11.8, see\n      # https://github.com/conda-forge/conda-forge-pinning-feedstock/pull/7472\n      # must be last due to how cuda_compiler ordering in that migrator works\n      - 11.8\n  commit_message: |\n    Upgrade to CUDA 12.9\n    \n    CUDA 12.8 added support for architectures `sm_100`, `sm_101` and `sm_120`,\n    while CUDA 12.9 further added `sm_103` and `sm_121`. To build for these,\n    maintainers will need to modify their existing list of specified architectures\n    (e.g. `CMAKE_CUDA_ARCHITECTURES`, `TORCH_CUDA_ARCH_LIST`, etc.)\n    for their package. A good balance between broad support and storage\n    footprint (resp. compilation time) is to add `sm_100` and `sm_120`.\n    \n    Since CUDA 12.8, the conda-forge nvcc package now sets `CUDAARCHS` and\n    `TORCH_CUDA_ARCH_LIST` in its activation script to a string containing all\n    of the supported real architectures plus the virtual architecture of the\n    latest. Recipes for packages who use these variables to control their build\n    but do not want to build for all supported architectures will need to override\n    these variables in their build script.\n    \n    ref: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#new-features\n\ncuda_compiler_version:         # [((linux and (x86_64 or aarch64)) or win64) and os.environ.get(\"CF_CUDA_ENABLED\", \"False\") == \"True\"]\n  - 12.9                       # [((linux and (x86_64 or aarch64)) or win64) and os.environ.get(\"CF_CUDA_ENABLED\", \"False\") == \"True\"]\n\ncuda_compiler_version_min:     # [((linux and (x86_64 or aarch64)) or win64) and os.environ.get(\"CF_CUDA_ENABLED\", \"False\") == \"True\"]\n  - 12.9                       # [((linux and (x86_64 or aarch64)) or win64) and os.environ.get(\"CF_CUDA_ENABLED\", \"False\") == \"True\"]\n\nc_compiler_version:            # [(linux and (x86_64 or aarch64)) and os.environ.get(\"CF_CUDA_ENABLED\", \"False\") == \"True\"]\n  - 14                         # [(linux and (x86_64 or aarch64)) and os.environ.get(\"CF_CUDA_ENABLED\", \"False\") == \"True\"]\n\ncxx_compiler_version:          # [(linux and (x86_64 or aarch64)) and os.environ.get(\"CF_CUDA_ENABLED\", \"False\") == \"True\"]\n  - 14                         # [(linux and (x86_64 or aarch64)) and os.environ.get(\"CF_CUDA_ENABLED\", \"False\") == \"True\"]\n\nfortran_compiler_version:      # [(linux and (x86_64 or aarch64)) and os.environ.get(\"CF_CUDA_ENABLED\", \"False\") == \"True\"]\n  - 14                         # [(linux and (x86_64 or aarch64)) and os.environ.get(\"CF_CUDA_ENABLED\", \"False\") == \"True\"]\n",
    "cuda129"
  ],
  "class": "MigrationYaml",
  "kwargs": {
    "automerge": false,
    "build_number": 1,
    "bump_number": 1,
    "check_solvable": false,
    "commit_message": "Upgrade to CUDA 12.9\n\nCUDA 12.8 added support for architectures `sm_100`, `sm_101` and `sm_120`,\nwhile CUDA 12.9 further added `sm_103` and `sm_121`. To build for these,\nmaintainers will need to modify their existing list of specified architectures\n(e.g. `CMAKE_CUDA_ARCHITECTURES`, `TORCH_CUDA_ARCH_LIST`, etc.)\nfor their package. A good balance between broad support and storage\nfootprint (resp. compilation time) is to add `sm_100` and `sm_120`.\n\nSince CUDA 12.8, the conda-forge nvcc package now sets `CUDAARCHS` and\n`TORCH_CUDA_ARCH_LIST` in its activation script to a string containing all\nof the supported real architectures plus the virtual architecture of the\nlatest. Recipes for packages who use these variables to control their build\nbut do not want to build for all supported architectures will need to override\nthese variables in their build script.\n\nref: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#new-features\n",
    "conda_forge_yml_patches": null,
    "cycles": {
      "__set__": true,
      "elements": []
    },
    "effective_graph": {
      "__nx_digraph__": true,
      "node_link_data": {
        "directed": true,
        "graph": {
          "outputs_lut": {},
          "strong_exports": {
            "__set__": true,
            "elements": [
              "c_compiler_stub",
              "c_stdlib_stub",
              "cuda_compiler_stub",
              "cxx_compiler_stub",
              "fortran_compiler_stub"
            ]
          }
        },
        "links": [],
        "multigraph": false,
        "nodes": []
      }
    },
    "force_pr_after_solver_attempts": 10,
    "graph": {
      "__nx_digraph__": true,
      "node_link_data": {
        "directed": true,
        "graph": {
          "outputs_lut": {},
          "strong_exports": {
            "__set__": true,
            "elements": [
              "c_compiler_stub",
              "c_stdlib_stub",
              "cuda_compiler_stub",
              "cxx_compiler_stub",
              "fortran_compiler_stub"
            ]
          }
        },
        "links": [],
        "multigraph": false,
        "nodes": []
      }
    },
    "ignored_deps_per_node": null,
    "kind": "version",
    "longterm": false,
    "max_solver_attempts": 3,
    "migration_number": 1,
    "ordering": {
      "cuda_compiler_version": [
        12.4,
        12.6,
        12.8,
        "None",
        12.9,
        11.8
      ]
    },
    "override_cbc_keys": [
      "cuda_compiler_stub"
    ],
    "package_names": {
      "__set__": true,
      "elements": [
        "cuda_compiler_stub"
      ]
    },
    "paused": false,
    "piggy_back_migrations": [
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "CrossCompilationForARMAndPower",
        "kwargs": {},
        "name": "CrossCompilationForARMAndPower"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "StdlibMigrator",
        "kwargs": {},
        "name": "StdlibMigrator"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "CondaForgeYAMLCleanup",
        "kwargs": {},
        "name": "CondaForgeYAMLCleanup"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "Jinja2VarsCleanup",
        "kwargs": {},
        "name": "Jinja2VarsCleanup"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "DuplicateLinesCleanup",
        "kwargs": {},
        "name": "DuplicateLinesCleanup"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "PipMigrator",
        "kwargs": {},
        "name": "PipMigrator"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "LicenseMigrator",
        "kwargs": {},
        "name": "LicenseMigrator"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "ExtraJinja2KeysCleanup",
        "kwargs": {},
        "name": "ExtraJinja2KeysCleanup"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "NoCondaInspectMigrator",
        "kwargs": {},
        "name": "NoCondaInspectMigrator"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "MPIPinRunAsBuildCleanup",
        "kwargs": {},
        "name": "MPIPinRunAsBuildCleanup"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "PyPIOrgMigrator",
        "kwargs": {},
        "name": "PyPIOrgMigrator"
      },
      {
        "__mini_migrator__": true,
        "args": [],
        "class": "CombineV1ConditionsMigrator",
        "kwargs": {},
        "name": "CombineV1ConditionsMigrator"
      }
    ],
    "pr_limit": 2,
    "primary_key": "cuda_compiler_version",
    "top_level": {
      "__set__": true,
      "elements": []
    },
    "total_graph": null
  },
  "name": "cuda129"
}